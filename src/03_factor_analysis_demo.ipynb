{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Factor Analysis on Financial and Economic Time Series\n",
    "\n",
    "Factor Analysis and Principal Component Analysis on Financial and Economic Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running this on Colab, make sure to install the following packages using pip.\n",
    "# On you're own computer, I recommend using conda or mamba.\n",
    "\n",
    "# !pip install pandas-datareader\n",
    "# !pip install yfinance\n",
    "\n",
    "# !conda install pandas-datareader\n",
    "# !conda install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "import sklearn.decomposition\n",
    "import statsmodels.multivariate.pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading macroeconomic and financial data from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_series_long_names = {\n",
    "    \"BAMLH0A0HYM2\": \"ICE BofA US High Yield Index Option-Adjusted Spread\",\n",
    "    \"NASDAQCOM\": \"NASDAQ Composite Index\",\n",
    "    \"RIFSPPFAAD90NB\": \"90-Day AA Financial Commercial Paper Interest Rate\",\n",
    "    \"TB3MS\": \"3-Month Treasury Bill Secondary Market Rate\",\n",
    "    \"DGS10\": \"Market Yield on U.S. Treasury Securities at 10-Year Constant Maturity\",\n",
    "    \"VIXCLS\": \"CBOE Volatility Index: VIX\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_series_short_names = {\n",
    "    \"BAMLH0A0HYM2\": \"High Yield Index OAS\",\n",
    "    \"NASDAQCOM\": \"NASDAQ\",\n",
    "    \"RIFSPPFAAD90NB\": \"90-Day AA Fin CP\",\n",
    "    \"TB3MS\": \"3-Month T-Bill\",\n",
    "    \"DGS10\": \"10-Year Treasury\",\n",
    "    \"VIXCLS\": \"VIX\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"1980-01-01\")\n",
    "end_date = pd.to_datetime(\"today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.get_data_fred(fred_series_short_names.keys(), start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an aside about reading and writing data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR / \"fred_panel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(DATA_DIR / \"fred_panel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(DATA_DIR / \"fred_panel.csv\", parse_dates=[\"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(DATA_DIR / \"fred_panel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_DIR / \"fred_panel.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dff.rename(columns=fred_series_short_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced panel? Mixed frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"3-Month T-Bill\"].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a daily version of this series. See here: https://fred.stlouisfed.org/categories/22\n",
    "\n",
    "We will end up using this series: https://fred.stlouisfed.org/series/DTB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_series_short_names = {\n",
    "    \"BAMLH0A0HYM2\": \"High Yield Index OAS\",\n",
    "    \"NASDAQCOM\": \"NASDAQ\",\n",
    "    \"RIFSPPFAAD90NB\": \"90-Day AA Fin CP\",\n",
    "    \"DTB3\": \"3-Month T-Bill\",\n",
    "    \"DGS10\": \"10-Year Treasury\",\n",
    "    \"VIXCLS\": \"VIX\",\n",
    "}\n",
    "df = pdr.get_data_fred(fred_series_short_names.keys(), start=start_date, end=end_date)\n",
    "df = df.rename(columns=fred_series_short_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and Normalizing the data\n",
    "\n",
    "What is transformation and normalization? Are these different things?\n",
    "\n",
    " - Why would one transform data? What is feature engineering?\n",
    " - What is normalization?\n",
    "\n",
    "What does stationarity mean? See the the following plots. Some of these variable are stationary. Other are not? Why is this a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"NASDAQ\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some transformations like those used in the OFR Financial Stress Index: https://www.financialresearch.gov/financial-stress-index/files/indicators/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.DataFrame().reindex_like(df)\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NASDAQ\"].rolling(250).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NASDAQ\"].rolling(250).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'High Yield Index OAS': Leave as is\n",
    "dfn[\"High Yield Index OAS\"] = df[\"High Yield Index OAS\"]\n",
    "dfn[\"CP - Treasury Spread, 3m\"] = df[\"90-Day AA Fin CP\"] - df[\"3-Month T-Bill\"]\n",
    "# 'NASDAQ':  # We're using something different, but still apply rolling mean transformation\n",
    "dfn[\"NASDAQ\"] = np.log(df[\"NASDAQ\"]) - np.log(df[\"NASDAQ\"].rolling(250).mean())\n",
    "dfn[\"10-Year Treasury\"] = (\n",
    "    df[\"10-Year Treasury\"] - df[\"10-Year Treasury\"].rolling(250).mean()\n",
    ")\n",
    "# 'VIX': Leave as is\n",
    "dfn[\"VIX\"] = df[\"VIX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = dfn.drop(columns=[\"90-Day AA Fin CP\", \"3-Month T-Bill\"])\n",
    "dfn = dfn.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished with our transformations. Now, let's normalize. First, why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, normalize each column,\n",
    "$$\n",
    "z = \\frac{x - \\bar x}{\\text{std}(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = (dfn - dfn.mean()) / dfn.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(dfn, module=\"scikitlearn\"):\n",
    "    if module == \"statsmodels\":\n",
    "        _pc1, _loadings, projection, rsquare, _, _, _ = (\n",
    "            statsmodels.multivariate.pca.pca(\n",
    "                dfn,\n",
    "                ncomp=1,\n",
    "                standardize=True,\n",
    "                demean=True,\n",
    "                normalize=True,\n",
    "                gls=False,\n",
    "                weights=None,\n",
    "                method=\"svd\",\n",
    "            )\n",
    "        )\n",
    "        _loadings = _loadings[\"comp_0\"]\n",
    "        loadings = np.std(_pc1) * _loadings\n",
    "        pc1 = _pc1 / np.std(_pc1)\n",
    "        pc1 = pc1.rename(columns={\"comp_0\": \"PC1\"})[\"PC1\"]\n",
    "\n",
    "    elif module == \"scikitlearn\":\n",
    "        pca = sklearn.decomposition.PCA(n_components=1)\n",
    "        _pc1 = pd.Series(pca.fit_transform(dfn)[:, 0], index=dfn.index, name=\"PC1\")\n",
    "        _loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "        _loadings = pd.Series(_loadings[:, 0], index=dfn.columns)\n",
    "\n",
    "        loadings = np.std(_pc1) * _loadings\n",
    "        pc1 = _pc1 / np.std(_pc1)\n",
    "        pc1.name = \"PC1\"\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    loadings.name = \"loadings\"\n",
    "\n",
    "    return pc1, loadings\n",
    "\n",
    "\n",
    "def stacked_plot(df, filename=None):\n",
    "    \"\"\"\n",
    "    df=category_contributions\n",
    "    # category_contributions.sum(axis=1).plot()\n",
    "    \"\"\"\n",
    "\n",
    "    df_pos = df[df >= 0]\n",
    "    df_neg = df[df < 0]\n",
    "\n",
    "    alpha = 0.3\n",
    "    linewidth = 0.5\n",
    "\n",
    "    ax = df_pos.plot.area(alpha=alpha, linewidth=linewidth, legend=False)\n",
    "    pc1 = df.sum(axis=1)\n",
    "    pc1.name = \"pc1\"\n",
    "    pc1.plot(color=\"Black\", label=\"pc1\", linewidth=1)\n",
    "\n",
    "    plt.legend()\n",
    "    ax.set_prop_cycle(None)\n",
    "    df_neg.plot.area(\n",
    "        alpha=alpha, ax=ax, linewidth=linewidth, legend=False, ylim=(-3, 3)\n",
    "    )\n",
    "    # recompute the ax.dataLim\n",
    "    ax.relim()\n",
    "    # update ax.viewLim using the new dataLim\n",
    "    ax.autoscale()\n",
    "    # ax.set_ylabel('Standard Deviations')\n",
    "    # ax.set_ylim(-3,4)\n",
    "    # ax.set_ylim(-30,30)\n",
    "\n",
    "    if filename is not None:\n",
    "        filename = Path(filename)\n",
    "        figure = plt.gcf()  # get current figure\n",
    "        figure.set_size_inches(8, 6)\n",
    "        plt.savefig(filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1, loadings = pca(dfn, module=\"scikitlearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(dfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare solutions from two different packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(sa, sb):\n",
    "    return np.sqrt(np.mean((sa - sb) ** 2))\n",
    "\n",
    "\n",
    "pc1_sk, loadings_sk = pca(dfn, module=\"scikitlearn\")\n",
    "pc1_sm, loadings_sm = pca(dfn, module=\"statsmodels\")\n",
    "root_mean_squared_error(pc1_sm, pc1_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis of a Panel of Stock Returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data for multiple tickers\n",
    "# Note: yfinance may return different structures depending on version and number of tickers\n",
    "sample = yf.download(\n",
    "    \"SPY AAPL MSFT\", start=\"2017-01-01\", end=\"2017-04-30\", progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the structure of the downloaded data\n",
    "print(\n",
    "    \"Sample columns:\",\n",
    "    sample.columns.tolist() if hasattr(sample, \"columns\") else \"No columns attribute\",\n",
    ")\n",
    "print(\n",
    "    \"Sample shape:\", sample.shape if hasattr(sample, \"shape\") else \"No shape attribute\"\n",
    ")\n",
    "if hasattr(sample, \"columns\") and isinstance(sample.columns, pd.MultiIndex):\n",
    "    print(\"Column levels:\", sample.columns.levels)\n",
    "    print(\"First level values:\", sample.columns.levels[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When downloading multiple tickers, yfinance returns a DataFrame with MultiIndex columns\n",
    "# The first level is the data type (e.g., 'Adj Close'), the second level is the ticker\n",
    "# Display the adjusted close prices for all tickers\n",
    "adj_close_data = (\n",
    "    sample[\"Adj Close\"] if \"Adj Close\" in sample.columns.get_level_values(0) else sample\n",
    ")\n",
    "adj_close_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    \"AAPL\",\n",
    "    \"ABBV\",\n",
    "    \"ABT\",\n",
    "    \"ACN\",\n",
    "    \"ADP\",\n",
    "    \"ADSK\",\n",
    "    \"AES\",\n",
    "    \"AET\",\n",
    "    \"AFL\",\n",
    "    \"AMAT\",\n",
    "    \"AMGN\",\n",
    "    \"AMZN\",\n",
    "    \"APA\",\n",
    "    \"APHA\",\n",
    "    \"APD\",\n",
    "    \"APTV\",\n",
    "    \"ARE\",\n",
    "    \"ASML\",\n",
    "    \"ATVI\",\n",
    "    \"AXP\",\n",
    "    \"BA\",\n",
    "    \"BAC\",\n",
    "    \"BAX\",\n",
    "    \"BDX\",\n",
    "    \"BIIB\",\n",
    "    \"BK\",\n",
    "    \"BKNG\",\n",
    "    \"BMY\",\n",
    "    \"BRKB\",\n",
    "    \"BRK.A\",\n",
    "    \"COG\",\n",
    "    \"COST\",\n",
    "    \"CPB\",\n",
    "    \"CRM\",\n",
    "    \"CSCO\",\n",
    "    \"CVS\",\n",
    "    \"DAL\",\n",
    "    \"DD\",\n",
    "    \"DHR\",\n",
    "    \"DIS\",\n",
    "    \"DOW\",\n",
    "    \"DUK\",\n",
    "    \"EMR\",\n",
    "    \"EPD\",\n",
    "    \"EQT\",\n",
    "    \"ESRT\",\n",
    "    \"EXPD\",\n",
    "    \"FFIV\",\n",
    "    \"FLS\",\n",
    "    \"FLT\",\n",
    "    \"FRT\",\n",
    "    \"GE\",\n",
    "    \"GILD\",\n",
    "    \"GOOGL\",\n",
    "    \"GOOG\",\n",
    "    \"GS\",\n",
    "    \"HAL\",\n",
    "    \"HD\",\n",
    "    \"HON\",\n",
    "    \"IBM\",\n",
    "    \"INTC\",\n",
    "    \"IP\",\n",
    "    \"JNJ\",\n",
    "    \"JPM\",\n",
    "    \"KEY\",\n",
    "    \"KHC\",\n",
    "    \"KIM\",\n",
    "    \"KO\",\n",
    "    \"LLY\",\n",
    "    \"LMT\",\n",
    "    \"LOW\",\n",
    "    \"MCD\",\n",
    "    \"MCHP\",\n",
    "    \"MDT\",\n",
    "    \"MMM\",\n",
    "    \"MO\",\n",
    "    \"MRK\",\n",
    "    \"MSFT\",\n",
    "    \"MTD\",\n",
    "    \"NEE\",\n",
    "    \"NFLX\",\n",
    "    \"NKE\",\n",
    "    \"NOV\",\n",
    "    \"ORCL\",\n",
    "    \"OXY\",\n",
    "    \"PEP\",\n",
    "    \"PFE\",\n",
    "    \"PG\",\n",
    "    \"RTN\",\n",
    "    \"RTX\",\n",
    "    \"SBUX\",\n",
    "    \"SHW\",\n",
    "    \"SLB\",\n",
    "    \"SO\",\n",
    "    \"SPG\",\n",
    "    \"STT\",\n",
    "    \"T\",\n",
    "    \"TGT\",\n",
    "    \"TXN\",\n",
    "    \"UNH\",\n",
    "    \"UPS\",\n",
    "    \"USB\",\n",
    "    \"UTX\",\n",
    "    \"V\",\n",
    "    \"VZ\",\n",
    "    \"WMT\",\n",
    "    \"XOM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers = \" \".join(tickers)\n",
    "all_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\n",
    "    all_tickers, start=\"1980-01-01\", end=pd.to_datetime(\"today\"), progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Close\"][\"AAPL\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_many_nas = [\n",
    "    \"BRK.A\",\n",
    "    \"APHA\",\n",
    "    \"UTX\",\n",
    "    \"RTN\",\n",
    "    \"COG\",\n",
    "    \"BRKB\",\n",
    "    \"ATVI\",\n",
    "    \"FLT\",\n",
    "    \"DOW\",\n",
    "    \"KHC\",\n",
    "    \"V\",\n",
    "    \"APTV\",\n",
    "    \"ABBV\",\n",
    "    \"ESRT\",\n",
    "]\n",
    "df = data[\"Close\"]\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "df = df.drop(columns=cols_with_many_nas, errors=\"ignore\")\n",
    "print(f\"After dropping columns: {df.shape}\")\n",
    "df = df.dropna()\n",
    "print(f\"After first dropna: {df.shape}\")\n",
    "df = df.pct_change()\n",
    "print(f\"After pct_change: {df.shape}\")\n",
    "df = df.dropna()\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "\n",
    "# If DataFrame is empty, use a smaller date range or fewer tickers\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty! Trying with fewer tickers and recent data...\")\n",
    "    simple_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"]\n",
    "    data = yf.download(\n",
    "        \" \".join(simple_tickers),\n",
    "        start=\"2020-01-01\",\n",
    "        end=pd.to_datetime(\"today\"),\n",
    "        progress=False,\n",
    "    )\n",
    "    df = data[\"Close\"].pct_change().dropna()\n",
    "    print(f\"New shape with simple tickers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AAPL\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    pc1, loadings = pca(df, module=\"scikitlearn\")\n",
    "    print(f\"PCA completed successfully. PC1 shape: {pc1.shape}\")\n",
    "else:\n",
    "    print(\"Cannot run PCA on empty DataFrame!\")\n",
    "    pc1 = pd.Series(dtype=float)\n",
    "    loadings = pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pc1.empty:\n",
    "    pc1.plot()\n",
    "else:\n",
    "    print(\"No data to plot for PC1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pc1.empty:\n",
    "    pc1.cumsum().plot()\n",
    "else:\n",
    "    print(\"No data to plot for cumulative PC1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
